{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <span style=\"color:indigo\">Machine Learning e Inferencia Bayesiana</span> </center> \n",
    "\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Centro_Universitario_del_Guadalajara_Logo.png/640px-Centro_Universitario_del_Guadalajara_Logo.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</center>\n",
    "    \n",
    "<center> <span style=\"color:DarkBlue\">  Tema 13: Redes neuronales, clasificacion </span>  </center>\n",
    "<center> <span style=\"color:Blue\"> M. en C. Iv√°n A. Toledano Ju√°rez </span>  </center>\n",
    "\n",
    "# Clasificaci√≥n con PyTorch\n",
    "\n",
    "Este notebook est√° basado en las notas de **MRDBourke** y utiliza datos del famoso dataset del **[Titanic](https://www.kaggle.com/competitions/titanic/overview)** disponible en Kaggle.  \n",
    "El objetivo es construir un modelo de **clasificaci√≥n binaria** con **PyTorch**, que prediga la probabilidad de supervivencia de los pasajeros.\n",
    "\n",
    "## Objetivo\n",
    "Entrenar una red neuronal simple que aprenda a clasificar a los pasajeros del Titanic seg√∫n las caracter√≠sticas disponibles, estimando si **sobrevivieron (1)** o **no sobrevivieron (0)**.\n",
    "\n",
    "## Variables del dataset\n",
    "\n",
    "| Variable | Descripci√≥n | Valores posibles |\n",
    "|-----------|--------------|------------------|\n",
    "| `survival` | Supervivencia | 0 = No, 1 = S√≠ |\n",
    "| `pclass` | Clase del boleto | 1 = 1¬™, 2 = 2¬™, 3 = 3¬™ |\n",
    "| `sex` | Sexo | ‚Äî |\n",
    "| `age` | Edad (en a√±os) | ‚Äî |\n",
    "| `sibsp` | N¬∫ de hermanos / c√≥nyuges a bordo | ‚Äî |\n",
    "| `parch` | N¬∫ de padres / hijos a bordo | ‚Äî |\n",
    "| `ticket` | N√∫mero de boleto | ‚Äî |\n",
    "| `fare` | Tarifa pagada | ‚Äî |\n",
    "| `cabin` | N√∫mero de cabina | ‚Äî |\n",
    "| `embarked` | Puerto de embarque | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "---\n",
    "\n",
    "A lo largo del notebook se realizar√° el **preprocesamiento de datos**, la **construcci√≥n del modelo**, y la **evaluaci√≥n de su desempe√±o** mediante m√©tricas de clasificaci√≥n como **exactitud** y **matriz de confusi√≥n**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass     Sex   Age  SibSp  Parch     Fare Cabin Embarked  Survived\n",
      "0       3    male  22.0      1      0   7.2500   NaN        S         0\n",
      "1       1  female  38.0      1      0  71.2833   C85        C         1\n",
      "2       3  female  26.0      0      0   7.9250   NaN        S         1\n",
      "3       1  female  35.0      1      0  53.1000  C123        S         1\n",
      "4       3    male  35.0      0      0   8.0500   NaN        S         0\n",
      "Shape(Train) (891, 9)\n",
      "Shape(test) (418, 8)\n"
     ]
    }
   ],
   "source": [
    "df_titanic_train = pd.read_csv(\"titanic_data/train.csv\")\n",
    "df_titanic_test = pd.read_csv(\"titanic_data/test.csv\")\n",
    "\n",
    "# Algunas variables no son necesarias\n",
    "\n",
    "variables = ['Pclass','Sex', 'Age', 'SibSp','Parch', 'Fare', 'Cabin', 'Embarked','Survived']\n",
    "variables_2 = variables.copy()\n",
    "variables_2.remove('Survived')\n",
    "\n",
    "df_titanic_train = df_titanic_train[variables]\n",
    "df_titanic_test = df_titanic_test[variables_2]\n",
    "\n",
    "\n",
    "print(df_titanic_train.head(5))\n",
    "\n",
    "print('Shape(Train)',df_titanic_train.shape)\n",
    "print('Shape(test)',df_titanic_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Pclass    891 non-null    int64  \n",
      " 1   Sex       891 non-null    object \n",
      " 2   Age       714 non-null    float64\n",
      " 3   SibSp     891 non-null    int64  \n",
      " 4   Parch     891 non-null    int64  \n",
      " 5   Fare      891 non-null    float64\n",
      " 6   Cabin     204 non-null    object \n",
      " 7   Embarked  889 non-null    object \n",
      " 8   Survived  891 non-null    int64  \n",
      "dtypes: float64(2), int64(4), object(3)\n",
      "memory usage: 62.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_titanic_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "Pclass      0\n",
       "Sex         0\n",
       "Age       177\n",
       "SibSp       0\n",
       "Parch       0\n",
       "Fare        0\n",
       "Cabin     687\n",
       "Embarked    2\n",
       "Survived    0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores nulos\n",
    "df_titanic_train.isna().sum().to_frame().head(83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "Pclass      0\n",
       "Sex         0\n",
       "Age        86\n",
       "SibSp       0\n",
       "Parch       0\n",
       "Fare        1\n",
       "Cabin     327\n",
       "Embarked    0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores nulos\n",
    "df_titanic_test.isna().sum().to_frame().head(83)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ignorar la variable Cabin. Para la variable de edad, (... podr√≠amos rellenarlo con la media .... o no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Pclass','Sex', 'Age', 'SibSp','Parch', 'Fare', 'Embarked','Survived']\n",
    "columns_2 = columns.copy()\n",
    "columns_2.remove('Survived')\n",
    "\n",
    "df2_titanic_train = df_titanic_train.copy()\n",
    "df2_titanic_test = df_titanic_test.copy()\n",
    "\n",
    "df2_titanic_train = df2_titanic_train[columns]\n",
    "df2_titanic_test = df2_titanic_test[columns_2]\n",
    "\n",
    "df2_titanic_train['Age'] = df2_titanic_train['Age'].fillna(df2_titanic_train['Age'].mean())\n",
    "df2_titanic_test['Age'] = df2_titanic_test['Age'].fillna(df2_titanic_test['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categoricas y num√©ricas\n",
    "\n",
    "categorical = df2_titanic_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical = df2_titanic_train.select_dtypes(include='number').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables dummy con pandas\n",
    "\n",
    "df3_titanic_train = df2_titanic_train.copy()\n",
    "df3_titanic_test = df2_titanic_test.copy()\n",
    "\n",
    "\n",
    "for element in categorical:\n",
    "    tab_dummy = pd.get_dummies(df3_titanic_train[element],prefix=element, dtype=int)\n",
    "    data_new = df3_titanic_train.join(tab_dummy)\n",
    "    df3_titanic_train = data_new\n",
    "    \n",
    "for element in categorical:\n",
    "    tab_dummy = pd.get_dummies(df3_titanic_test[element],prefix=element, dtype=int)\n",
    "    data_new = df3_titanic_test.join(tab_dummy)\n",
    "    df3_titanic_test = data_new\n",
    "\n",
    "# Quitamos las columnas redundantes\n",
    "to_keep = [element for element in df3_titanic_train.columns if element not in categorical]\n",
    "to_keep_2 = [element for element in df3_titanic_test.columns if element not in categorical]\n",
    "\n",
    "df3_titanic_train = df3_titanic_train[to_keep]\n",
    "df3_titanic_test = df3_titanic_test[to_keep_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass        Age  SibSp  Parch     Fare  Survived  Sex_female  Sex_male  \\\n",
       "0         3  22.000000      1      0   7.2500         0           0         1   \n",
       "1         1  38.000000      1      0  71.2833         1           1         0   \n",
       "2         3  26.000000      0      0   7.9250         1           1         0   \n",
       "3         1  35.000000      1      0  53.1000         1           1         0   \n",
       "4         3  35.000000      0      0   8.0500         0           0         1   \n",
       "..      ...        ...    ...    ...      ...       ...         ...       ...   \n",
       "886       2  27.000000      0      0  13.0000         0           0         1   \n",
       "887       1  19.000000      0      0  30.0000         1           1         0   \n",
       "888       3  29.699118      1      2  23.4500         0           1         0   \n",
       "889       1  26.000000      0      0  30.0000         1           0         1   \n",
       "890       3  32.000000      0      0   7.7500         0           0         1   \n",
       "\n",
       "     Embarked_C  Embarked_Q  Embarked_S  \n",
       "0             0           0           1  \n",
       "1             1           0           0  \n",
       "2             0           0           1  \n",
       "3             0           0           1  \n",
       "4             0           0           1  \n",
       "..          ...         ...         ...  \n",
       "886           0           0           1  \n",
       "887           0           0           1  \n",
       "888           0           0           1  \n",
       "889           1           0           0  \n",
       "890           0           1           0  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_titanic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features y class\n",
    "X_list = to_keep.copy()\n",
    "X_list.remove('Survived')\n",
    "Y_list = 'Survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes a array\n",
    "\n",
    "X_array = df3_titanic_train[X_list].to_numpy()\n",
    "Y_array = df3_titanic_train[Y_list].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.        , 22.        ,  1.        , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 1.        , 38.        ,  1.        , ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 3.        , 26.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ...,\n",
       "       [ 3.        , 29.69911765,  1.        , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 1.        , 26.        ,  0.        , ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 3.        , 32.        ,  0.        , ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura de una red neuronal para clasificaci√≥n\n",
    "\n",
    "| Hiperpar√°metro | Clasificaci√≥n binaria | Clasificaci√≥n multiclase |\n",
    "| --- | --- | --- |\n",
    "| Capa de entrada | El mismo que el n√∫mero de variables de entrada | Igual que clasificaci√≥n binaria|\n",
    "| Capas ocultas | Depende del problema. Te√≥ricamente puede ir de 1 a infinito | Igual que clasificaci√≥n binaria|\n",
    "| Neuronas por capa oculta | Depende del probleme. Usualmente entre 10 y 512 | Igual que clasificaci√≥n binaria|\n",
    "| Capas de salida | 1 (una por clase) | Una por cada clase|\n",
    "| Funci√≥n de activaci√≥n de capas ocultas | T√≠pica: ReLU, pero puede ser cualquiera. | Igual que clasificaci√≥n binaria|\n",
    "| Funci√≥n de activaci√≥n de capa de salida | T√≠pica: Sigmoid | [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) |\n",
    "| Loss Function | [Binary crossentropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) | [Crossentropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)|\n",
    "| Optimizador | SGD, [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) | Igual que clasificaci√≥n binaria|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays a tensores, y sets de entrenamiento y validaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_array).type(torch.float)\n",
    "y = torch.from_numpy(Y_array).type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 179, 712, 179)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, # 20% test, 80% train\n",
    "                                                    random_state=42) # make the random split reproducible\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando el modelo\n",
    "\n",
    "Ya tenemos nuestros datos listos, as√≠ que es momento de **construir un modelo** de clasificaci√≥n utilizando **PyTorch**.  \n",
    "El proceso lo dividiremos en varios pasos clave:\n",
    "\n",
    "1. Configurar c√≥digo **agn√≥stico al dispositivo** (CPU o GPU).  \n",
    "2. Construir un modelo **subclasificando `nn.Module`**.  \n",
    "3. Definir una **funci√≥n de p√©rdida** y un **optimizador**.  \n",
    "4. Crear un **bucle de entrenamiento**.\n",
    "\n",
    "La buena noticia es que ya hemos seguido estos pasos antes (en el notebook anterior), solo que ahora los **ajustaremos para un problema de clasificaci√≥n**.\n",
    "\n",
    "---\n",
    "\n",
    "## Configuraci√≥n del dispositivo\n",
    "\n",
    "Comenzamos importando las librer√≠as necesarias y preparando el entorno para que el modelo pueda ejecutarse en **CPU o GPU**, seg√∫n disponibilidad. Si tu equipo tiene acceso a una GPU compatible, pytorch la utilizar√° autom√°ticamente. Esto permite que todo --datos, modelos y tensores -- se gestionesn en el dispositivo adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fijar el tipo de hardware\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([712, 10])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shapes de tensores\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creaci√≥n del modelo\n",
    "\n",
    "Queremos un modelo que reciba nuestros datos de entrada `X`(features) y produzca una predicci√≥n `y`, es decir, un tipo de problema supervisado. Para ello, definiremos una clase en python que,\n",
    "\n",
    "* Herede de `nn.module` (como todos los modelos de pytorch)\n",
    "* Cree dos capas lineales `nn.linear` en el constructor, con las dimensiones de entrada y salida adecuadas para nuestros datos.\n",
    "* Implemente un m√©todo `forward()` que defina la propagaci√≥n hacia adelante del modelo\n",
    "* Instanciamos el modelo y lo env√≠amos al dispositivo configurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelV0(\n",
       "  (layer_1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (layer_2): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Construimos la clase del modelo con la subclase nn.Module\n",
    "class ModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 2. Creamos las capas de entrada (lineales) capaces de manejar los features de entrada y clase de salida\n",
    "        self.layer_1 = nn.Linear(in_features=10, out_features=20) # toma 10 features (X), produce 20 features\n",
    "        self.layer_2 = nn.Linear(in_features=20, out_features=1) # toma 20 features, produce 1 feature (y)\n",
    "    \n",
    "    # 3. Definimos un m√©todo para la propagaci√≥n (forward)\n",
    "    def forward(self, x):\n",
    "        # Regresa la capa de salida de layer_2, un solo features, con el mismo shape que y\n",
    "        # El calculo pasa sobre layer_1 y luego su output es el input de layer_2\n",
    "        return self.layer_2(self.layer_1(x)) \n",
    "\n",
    "# 4. Creamos una instancia con el modelo y se manda al hardware\n",
    "model_0 = ModelV0().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera capa (`layer_1`) recibe 2 caracter√≠sticas de entrada (`in_features=2`) y produce 5 salidas (`out_features=5`). Estas 5 salidas se conocen como unidades ocultas, y permiten al modelo aprender **patrones m√°s complejos**.\n",
    "\n",
    "La segunda capa (`layer_2`) toma esas 5 caracter√≠sticas y las transforma en una √∫nica salida (`out_features=1`), que corresponden a la predicci√≥n del modelo.\n",
    "\n",
    "**NOTA**: El n√∫mero de unidades ocultas las elige uno. M√°s unidades podr√≠an capturar patrones m√°s complejos, pero tambi√©n pueden provocar sobreajuste y entrenamiento m√°s lento.\n",
    "\n",
    "## `nn.Sequential`\n",
    "\n",
    "El m√©todo `nn.Sequential()` ejecuta la propagaci√≥n hacia adelante en el orden en que aparecen las capas, simplificando la sintaxis cuando no se requieren pasos intermedios personalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (1): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se replica el modelV0\n",
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=10, out_features=20),\n",
    "    nn.Linear(in_features=20, out_features=1)\n",
    ").to(device)\n",
    "\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of predictions: 179, Shape: torch.Size([179, 1])\n",
      "Length of test samples: 179, Shape: torch.Size([179])\n",
      "\n",
      "First 10 predictions:\n",
      "tensor([[ -2.9761],\n",
      "        [ -2.1585],\n",
      "        [ -1.7118],\n",
      "        [ -5.3292],\n",
      "        [ -2.1719],\n",
      "        [-12.3164],\n",
      "        [ -1.9615],\n",
      "        [ -3.0549],\n",
      "        [ -1.6985],\n",
      "        [ -4.5233]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "First 10 test labels:\n",
      "tensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Hacemos predicciones con el modelo\n",
    "untrained_preds = model_0(X_test.to(device))\n",
    "print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\n",
    "print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\n",
    "print(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraci√≥n de la funci√≥n de p√©rdida y el optimizador\n",
    "\n",
    "Ya hemos configurado modelos, as√≠ que ahora toca definir **c√≥mo aprender√°**, a trav√©s de una **funci√≥n de p√©rdida** (*loss function*) y un **optimizador**.\n",
    "\n",
    "En el notebook previo ya usamos estos conceptos, pero es importante notar que **diferentes tipos de problemas requieren distintas funciones de p√©rdida.**\n",
    "\n",
    "---\n",
    "\n",
    "## Funci√≥n de p√©rdida\n",
    "\n",
    "La funci√≥n de p√©rdida (tambi√©n llamada *cost function*) mide **qu√© tan equivocadas son las predicciones del modelo**. Mientras m√°s alto sea su valor, peor est√° aprendiendo el modelo.  El entrenamiento consiste en **minimizar esta p√©rdida**.\n",
    "\n",
    "Ejemplos comunes:\n",
    "\n",
    "| Funci√≥n / Optimizador | Tipo de problema | C√≥digo en PyTorch |\n",
    "|------------------------|------------------|-------------------|\n",
    "| Stochastic Gradient Descent (SGD) | Clasificaci√≥n, regresi√≥n, muchos otros | `torch.optim.SGD()` |\n",
    "| Adam Optimizer | Clasificaci√≥n, regresi√≥n, muchos otros | `torch.optim.Adam()` |\n",
    "| Binary Cross Entropy (BCE) | Clasificaci√≥n binaria | `torch.nn.BCELoss()` o `torch.nn.BCEWithLogitsLoss()` |\n",
    "| Cross Entropy | Clasificaci√≥n multiclase | `torch.nn.CrossEntropyLoss()` |\n",
    "| Mean Absolute Error (MAE) / L1 Loss | Regresi√≥n | `torch.nn.L1Loss()` |\n",
    "| Mean Squared Error (MSE) / L2 Loss | Regresi√≥n | `torch.nn.MSELoss()` |\n",
    "\n",
    "---\n",
    "\n",
    "## Elecci√≥n para nuestro caso\n",
    "\n",
    "Como estamos trabajando con un **problema de clasificaci√≥n binaria**, la opci√≥n m√°s adecuada es usar una **p√©rdida de entrop√≠a cruzada binaria** (*binary cross entropy loss*).\n",
    "\n",
    "PyTorch ofrece dos versiones:\n",
    "\n",
    "- `torch.nn.BCELoss()`  \n",
    "  Calcula la entrop√≠a cruzada binaria entre las predicciones y las etiquetas.\n",
    "  \n",
    "- `torch.nn.BCEWithLogitsLoss()`  \n",
    "  Hace lo mismo, pero **integra internamente una funci√≥n sigmoide** (`nn.Sigmoid`).  \n",
    "  Esto la hace **m√°s estable num√©ricamente** y generalmente se recomienda sobre la anterior.\n",
    "\n",
    "> üí° **Recomendaci√≥n:**  \n",
    "> Usa `torch.nn.BCEWithLogitsLoss()` en la mayor√≠a de los casos de clasificaci√≥n binaria.  \n",
    "> Evita aplicar manualmente un `Sigmoid` si utilizas esta versi√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## Optimizador\n",
    "\n",
    "El optimizador es el algoritmo que **ajusta los pesos del modelo** para minimizar la p√©rdida.  Podemos usar el cl√°sico **descenso de gradiente estoc√°stico (SGD)** o el m√°s moderno **Adam**. Ambos funcionan bien, pero empezaremos con **SGD** para mayor claridad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n",
    "\n",
    "# Optimizador\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), \n",
    "                            lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©trica de evaluaci√≥n (accuracy)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calcula si dos tensores son iguales\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n",
    "\n",
    "Antes de realizar el loop de entrenamiento, veamos que sale del modelo al realizar una propagaci√≥n hacia adelante, usando los datos de validacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9761],\n",
       "        [-2.1585],\n",
       "        [-1.7118],\n",
       "        [-5.3292],\n",
       "        [-2.1719]], device='mps:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Los 5 primeros outputs\n",
    "y_logits = model_0(X_test.to(device))[:5]\n",
    "y_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el modelo todav√≠a **no ha sido entrenado**, sus salidas son esencialmente **valores aleatorios**.  Durante la **propagaci√≥n hacia adelante**, los datos pasan a trav√©s de las dos capas lineales definidas, las cuales aplican internamente la siguiente ecuaci√≥n:\n",
    "\n",
    "\\begin{equation}\n",
    "y = x \\cdot w^T + \\mathrm{bias}\n",
    "\\end{equation}\n",
    "\n",
    "Los valores resultantes $y$ de esta operaci√≥n, as√≠ como los que produce el modelo, se conocen como **_logits_**.  En un modelo puramente lineal, estos logits representar√≠an simplemente **valores num√©ricos** sin restricci√≥n en su rango (pueden ser negativos o positivos).\n",
    "\n",
    "Si aplicamos una **funci√≥n de activaci√≥n sigmoide** sobre ellos, podemos convertir dichos valores en **probabilidades** dentro del intervalo $(0, 1)$, lo que nos permite interpretar el resultado como:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{probabilidad de clase positiva} = \\sigma(y) = \\frac{1}{1 + e^{-y}}\n",
    "\\end{equation}\n",
    "\n",
    "De este modo, al establecer un **umbral (threshold)** ‚Äîpor ejemplo, 0.5‚Äî podemos transformar las probabilidades en una **clasificaci√≥n binaria**:\n",
    "\n",
    "- Si $\\sigma(y) \\ge 0.5$ ‚Üí clase **1 (positivo)**  \n",
    "- Si $\\sigma(y) < 0.5$ ‚Üí clase **0 (negativo)**\n",
    "\n",
    "> üí° **Nota:** En PyTorch, cuando se utiliza `nn.BCEWithLogitsLoss`, la funci√≥n sigmoide ya est√° incorporada dentro de la funci√≥n de p√©rdida, por lo que **no es necesario aplicarla manualmente** en la salida del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0485],\n",
       "        [0.1035],\n",
       "        [0.1529],\n",
       "        [0.0048],\n",
       "        [0.1023]], device='mps:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigmoid\n",
    "y_pred_probs = torch.sigmoid(y_logits)\n",
    "y_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.], device='mps:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redondeamos para obtener una clasificaci√≥n (threshold 0.5)\n",
    "#y_preds = torch.round(y_pred_probs)\n",
    "threshold = 0.5\n",
    "y_preds = (y_pred_probs >= threshold).float()\n",
    "\n",
    "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
    "\n",
    "# Checamos igualdad\n",
    "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
    "\n",
    "# Quitamos la dimensi√≥n extra\n",
    "y_preds.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]\n",
    "# Vemos que ahora si tenemos las etiquetas que queremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 3.08763, Accuracy: 62.36% | Test loss: 1.92369, Test acc: 41.34%\n",
      "Epoch: 10 | Loss: 1.29656, Accuracy: 62.36% | Test loss: 1.46634, Test acc: 43.02%\n",
      "Epoch: 20 | Loss: 0.71656, Accuracy: 64.47% | Test loss: 0.73196, Test acc: 69.27%\n",
      "Epoch: 30 | Loss: 0.76566, Accuracy: 67.13% | Test loss: 0.59660, Test acc: 73.18%\n",
      "Epoch: 40 | Loss: 0.77489, Accuracy: 62.36% | Test loss: 0.87012, Test acc: 62.57%\n",
      "Epoch: 50 | Loss: 0.66719, Accuracy: 63.62% | Test loss: 0.64311, Test acc: 74.30%\n",
      "Epoch: 60 | Loss: 0.67460, Accuracy: 64.19% | Test loss: 0.62090, Test acc: 74.86%\n",
      "Epoch: 70 | Loss: 0.65832, Accuracy: 63.76% | Test loss: 0.61154, Test acc: 73.18%\n",
      "Epoch: 80 | Loss: 0.64603, Accuracy: 64.04% | Test loss: 0.59991, Test acc: 74.86%\n",
      "Epoch: 90 | Loss: 0.63686, Accuracy: 64.19% | Test loss: 0.59219, Test acc: 74.30%\n",
      "Epoch: 100 | Loss: 0.62973, Accuracy: 64.33% | Test loss: 0.58683, Test acc: 72.63%\n",
      "Epoch: 110 | Loss: 0.62401, Accuracy: 65.17% | Test loss: 0.58295, Test acc: 73.18%\n",
      "Epoch: 120 | Loss: 0.61930, Accuracy: 65.31% | Test loss: 0.58000, Test acc: 73.74%\n",
      "Epoch: 130 | Loss: 0.61534, Accuracy: 65.73% | Test loss: 0.57763, Test acc: 74.30%\n",
      "Epoch: 140 | Loss: 0.61195, Accuracy: 65.59% | Test loss: 0.57563, Test acc: 73.18%\n",
      "Epoch: 150 | Loss: 0.60901, Accuracy: 66.15% | Test loss: 0.57386, Test acc: 73.18%\n",
      "Epoch: 160 | Loss: 0.60641, Accuracy: 66.29% | Test loss: 0.57226, Test acc: 73.18%\n",
      "Epoch: 170 | Loss: 0.60409, Accuracy: 66.43% | Test loss: 0.57075, Test acc: 73.18%\n",
      "Epoch: 180 | Loss: 0.60200, Accuracy: 66.43% | Test loss: 0.56933, Test acc: 73.18%\n",
      "Epoch: 190 | Loss: 0.60010, Accuracy: 66.57% | Test loss: 0.56797, Test acc: 73.18%\n",
      "Epoch: 200 | Loss: 0.59835, Accuracy: 66.57% | Test loss: 0.56666, Test acc: 73.18%\n",
      "Epoch: 210 | Loss: 0.59673, Accuracy: 66.99% | Test loss: 0.56540, Test acc: 72.63%\n",
      "Epoch: 220 | Loss: 0.59521, Accuracy: 66.99% | Test loss: 0.56419, Test acc: 72.63%\n",
      "Epoch: 230 | Loss: 0.59378, Accuracy: 66.85% | Test loss: 0.56303, Test acc: 72.63%\n",
      "Epoch: 240 | Loss: 0.59243, Accuracy: 66.85% | Test loss: 0.56191, Test acc: 72.63%\n",
      "Epoch: 250 | Loss: 0.59114, Accuracy: 66.85% | Test loss: 0.56083, Test acc: 73.18%\n",
      "Epoch: 260 | Loss: 0.58990, Accuracy: 66.85% | Test loss: 0.55979, Test acc: 73.18%\n",
      "Epoch: 270 | Loss: 0.58871, Accuracy: 66.99% | Test loss: 0.55879, Test acc: 73.18%\n",
      "Epoch: 280 | Loss: 0.58757, Accuracy: 66.99% | Test loss: 0.55782, Test acc: 73.18%\n",
      "Epoch: 290 | Loss: 0.58645, Accuracy: 66.99% | Test loss: 0.55689, Test acc: 73.18%\n",
      "Epoch: 300 | Loss: 0.58537, Accuracy: 66.99% | Test loss: 0.55599, Test acc: 73.18%\n",
      "Epoch: 310 | Loss: 0.58432, Accuracy: 67.13% | Test loss: 0.55511, Test acc: 74.30%\n",
      "Epoch: 320 | Loss: 0.58330, Accuracy: 67.13% | Test loss: 0.55426, Test acc: 74.30%\n",
      "Epoch: 330 | Loss: 0.58229, Accuracy: 67.13% | Test loss: 0.55344, Test acc: 74.30%\n",
      "Epoch: 340 | Loss: 0.58131, Accuracy: 67.13% | Test loss: 0.55264, Test acc: 74.30%\n",
      "Epoch: 350 | Loss: 0.58035, Accuracy: 67.13% | Test loss: 0.55186, Test acc: 74.86%\n",
      "Epoch: 360 | Loss: 0.57941, Accuracy: 66.99% | Test loss: 0.55111, Test acc: 75.42%\n",
      "Epoch: 370 | Loss: 0.57849, Accuracy: 66.99% | Test loss: 0.55037, Test acc: 75.42%\n",
      "Epoch: 380 | Loss: 0.57758, Accuracy: 66.99% | Test loss: 0.54964, Test acc: 75.42%\n",
      "Epoch: 390 | Loss: 0.57669, Accuracy: 66.99% | Test loss: 0.54894, Test acc: 75.98%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(88) # semilla aleatoria\n",
    "\n",
    "# N√∫mero de epocas\n",
    "epochs = 400\n",
    "\n",
    "# Poner los datos en el hardware target\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Loop de training y eval\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward propagation (el modelo regresa logits)\n",
    "    y_logits = model_0(X_train).squeeze() # squeeze para remover `1` dimension extra\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Se calcula loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss acepta los logits de salida\n",
    "                   y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Zero grad para el optimizador\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizador\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Evaluacion\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward \n",
    "        test_logits = model_0(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print cada 10 epocas\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejorando el modelo\n",
    "\n",
    "Una vez que el modelo b√°sico est√° funcionando, existen diversas estrategias para **mejorar su desempe√±o**.  Cada una de las siguientes t√©cnicas busca aumentar la capacidad del modelo para **aprender patrones m√°s complejos** o **ajustarse mejor a los datos**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. A√±adir m√°s capas\n",
    "Cada capa adicional puede incrementar la **capacidad de representaci√≥n** del modelo, permiti√©ndole aprender **patrones m√°s abstractos y no lineales**. Agregar m√°s capas hace que la red sea m√°s **profunda**, lo que da origen al t√©rmino *deep learning*.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. A√±adir m√°s neuronas ocultas\n",
    "De forma similar, aumentar el n√∫mero de **neuronas (unidades ocultas)** dentro de una capa puede mejorar la capacidad del modelo para capturar relaciones complejas entre las variables. Sin embargo, demasiadas neuronas pueden llevar al **sobreajuste (overfitting)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Entrenar por m√°s √©pocas\n",
    "Dar al modelo m√°s **√©pocas** (iteraciones completas sobre los datos) permite que los pesos se actualicen m√°s veces, lo que puede mejorar el rendimiento si el modelo a√∫n no ha convergido. Pero un n√∫mero excesivo de √©pocas tambi√©n puede causar **sobreajuste**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Cambiar la funci√≥n de activaci√≥n\n",
    "Los datos reales rara vez son lineales. Usar funciones de activaci√≥n **no lineales** (como ReLU, tanh o sigmoid) permite que el modelo aprenda relaciones m√°s complejas.  \n",
    "Por ejemplo:\n",
    "- `nn.ReLU()` ‚Üí com√∫n en redes profundas.  \n",
    "- `nn.Sigmoid()` ‚Üí √∫til en clasificaci√≥n binaria.  \n",
    "- `nn.Tanh()` ‚Üí centrada en 0, √∫til para ciertos tipos de datos.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Ajustar la tasa de aprendizaje\n",
    "La **tasa de aprendizaje** (`learning_rate`) controla qu√© tanto se ajustan los par√°metros en cada actualizaci√≥n.  \n",
    "- Si es **demasiado alta**, el modelo puede **oscilar o divergir**.  \n",
    "- Si es **demasiado baja**, el aprendizaje ser√° **muy lento** o se quedar√° estancado en un m√≠nimo local.  \n",
    "\n",
    "Encontrar un valor adecuado requiere **experimentaci√≥n o b√∫squeda sistem√°tica (grid/random search)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Cambiar la funci√≥n de p√©rdida\n",
    "Cada tipo de problema (clasificaci√≥n binaria, multiclase, regresi√≥n, etc.) requiere una **funci√≥n de p√©rdida diferente**.  \n",
    "Probar distintas opciones puede mejorar la estabilidad o precisi√≥n del aprendizaje.\n",
    "\n",
    "Ejemplo:\n",
    "- Clasificaci√≥n binaria ‚Üí `nn.BCEWithLogitsLoss()`\n",
    "- Clasificaci√≥n multiclase ‚Üí `nn.CrossEntropyLoss()`\n",
    "- Regresi√≥n ‚Üí `nn.MSELoss()` o `nn.L1Loss()`\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Transfer learning\n",
    "En lugar de entrenar un modelo desde cero, se puede **aprovechar un modelo preentrenado** en un problema similar y **ajustarlo (fine-tuning)** a los nuevos datos.  \n",
    "Esta t√©cnica es muy √∫til cuando se dispone de **pocos datos** o se trabaja con **dominios complejos**, como im√°genes o texto.\n",
    "\n",
    "---\n",
    "\n",
    "> **NOTA:** No existe una receta √∫nica para mejorar el modelo.  \n",
    "> La pr√°ctica m√°s com√∫n es **ajustar un hiperpar√°metro a la vez**, observar su impacto en la p√©rdida y en las m√©tricas de validaci√≥n, y repetir el proceso hasta encontrar un equilibrio entre **precisi√≥n y generalizaci√≥n**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelV1(\n",
       "  (layer_1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (layer_2): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (layer_3): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=10, out_features=20)\n",
    "        self.layer_2 = nn.Linear(in_features=20, out_features=20) # capa extra\n",
    "        self.layer_3 = nn.Linear(in_features=20, out_features=1)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        # z = self.layer_1(x)\n",
    "        # z = self.layer_2(z)\n",
    "        # z = self.layer_3(z)\n",
    "        # return z\n",
    "        return self.layer_3(self.layer_2(self.layer_1(x)))\n",
    "\n",
    "model_1 = ModelV1().to(device)\n",
    "model_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss() \n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.22616, Accuracy: 35.39% | Test loss: 0.87986, Test acc: 41.34%\n",
      "Epoch: 10 | Loss: 0.64346, Accuracy: 64.19% | Test loss: 0.59997, Test acc: 72.63%\n",
      "Epoch: 20 | Loss: 0.64012, Accuracy: 64.33% | Test loss: 0.59621, Test acc: 72.07%\n",
      "Epoch: 30 | Loss: 0.63408, Accuracy: 64.75% | Test loss: 0.59300, Test acc: 71.51%\n",
      "Epoch: 40 | Loss: 0.62924, Accuracy: 65.17% | Test loss: 0.59080, Test acc: 70.95%\n",
      "Epoch: 50 | Loss: 0.62541, Accuracy: 65.59% | Test loss: 0.58929, Test acc: 70.95%\n",
      "Epoch: 60 | Loss: 0.62230, Accuracy: 65.73% | Test loss: 0.58812, Test acc: 71.51%\n",
      "Epoch: 70 | Loss: 0.61971, Accuracy: 66.01% | Test loss: 0.58711, Test acc: 70.95%\n",
      "Epoch: 80 | Loss: 0.61752, Accuracy: 66.01% | Test loss: 0.58615, Test acc: 72.07%\n",
      "Epoch: 90 | Loss: 0.61565, Accuracy: 66.15% | Test loss: 0.58520, Test acc: 72.07%\n",
      "Epoch: 100 | Loss: 0.61402, Accuracy: 66.15% | Test loss: 0.58425, Test acc: 72.07%\n",
      "Epoch: 110 | Loss: 0.61258, Accuracy: 66.15% | Test loss: 0.58328, Test acc: 72.07%\n",
      "Epoch: 120 | Loss: 0.61129, Accuracy: 66.43% | Test loss: 0.58232, Test acc: 72.07%\n",
      "Epoch: 130 | Loss: 0.61011, Accuracy: 66.29% | Test loss: 0.58137, Test acc: 72.07%\n",
      "Epoch: 140 | Loss: 0.60903, Accuracy: 66.29% | Test loss: 0.58043, Test acc: 72.07%\n",
      "Epoch: 150 | Loss: 0.60801, Accuracy: 66.29% | Test loss: 0.57952, Test acc: 71.51%\n",
      "Epoch: 160 | Loss: 0.60705, Accuracy: 66.29% | Test loss: 0.57863, Test acc: 72.07%\n",
      "Epoch: 170 | Loss: 0.60612, Accuracy: 66.43% | Test loss: 0.57776, Test acc: 70.95%\n",
      "Epoch: 180 | Loss: 0.60523, Accuracy: 66.43% | Test loss: 0.57693, Test acc: 70.95%\n",
      "Epoch: 190 | Loss: 0.60437, Accuracy: 66.43% | Test loss: 0.57611, Test acc: 71.51%\n",
      "Epoch: 200 | Loss: 0.60353, Accuracy: 66.43% | Test loss: 0.57532, Test acc: 71.51%\n",
      "Epoch: 210 | Loss: 0.60271, Accuracy: 66.43% | Test loss: 0.57455, Test acc: 71.51%\n",
      "Epoch: 220 | Loss: 0.60190, Accuracy: 66.43% | Test loss: 0.57380, Test acc: 71.51%\n",
      "Epoch: 230 | Loss: 0.60111, Accuracy: 66.43% | Test loss: 0.57307, Test acc: 71.51%\n",
      "Epoch: 240 | Loss: 0.60033, Accuracy: 66.57% | Test loss: 0.57235, Test acc: 71.51%\n",
      "Epoch: 250 | Loss: 0.59956, Accuracy: 66.71% | Test loss: 0.57164, Test acc: 71.51%\n",
      "Epoch: 260 | Loss: 0.59880, Accuracy: 66.71% | Test loss: 0.57095, Test acc: 71.51%\n",
      "Epoch: 270 | Loss: 0.59805, Accuracy: 66.71% | Test loss: 0.57027, Test acc: 71.51%\n",
      "Epoch: 280 | Loss: 0.59731, Accuracy: 66.71% | Test loss: 0.56959, Test acc: 71.51%\n",
      "Epoch: 290 | Loss: 0.59657, Accuracy: 66.71% | Test loss: 0.56893, Test acc: 71.51%\n",
      "Epoch: 300 | Loss: 0.59584, Accuracy: 66.71% | Test loss: 0.56827, Test acc: 70.95%\n",
      "Epoch: 310 | Loss: 0.59512, Accuracy: 66.57% | Test loss: 0.56762, Test acc: 70.95%\n",
      "Epoch: 320 | Loss: 0.59441, Accuracy: 66.71% | Test loss: 0.56697, Test acc: 70.95%\n",
      "Epoch: 330 | Loss: 0.59370, Accuracy: 66.71% | Test loss: 0.56633, Test acc: 70.95%\n",
      "Epoch: 340 | Loss: 0.59299, Accuracy: 66.85% | Test loss: 0.56569, Test acc: 71.51%\n",
      "Epoch: 350 | Loss: 0.59230, Accuracy: 66.99% | Test loss: 0.56506, Test acc: 71.51%\n",
      "Epoch: 360 | Loss: 0.59160, Accuracy: 66.99% | Test loss: 0.56443, Test acc: 71.51%\n",
      "Epoch: 370 | Loss: 0.59091, Accuracy: 66.99% | Test loss: 0.56381, Test acc: 71.51%\n",
      "Epoch: 380 | Loss: 0.59023, Accuracy: 66.99% | Test loss: 0.56319, Test acc: 71.51%\n",
      "Epoch: 390 | Loss: 0.58955, Accuracy: 66.99% | Test loss: 0.56257, Test acc: 70.95%\n",
      "Epoch: 400 | Loss: 0.58887, Accuracy: 66.99% | Test loss: 0.56195, Test acc: 70.95%\n",
      "Epoch: 410 | Loss: 0.58820, Accuracy: 66.99% | Test loss: 0.56133, Test acc: 72.07%\n",
      "Epoch: 420 | Loss: 0.58753, Accuracy: 66.99% | Test loss: 0.56072, Test acc: 72.07%\n",
      "Epoch: 430 | Loss: 0.58686, Accuracy: 66.99% | Test loss: 0.56011, Test acc: 72.07%\n",
      "Epoch: 440 | Loss: 0.58620, Accuracy: 66.99% | Test loss: 0.55950, Test acc: 72.63%\n",
      "Epoch: 450 | Loss: 0.58553, Accuracy: 66.99% | Test loss: 0.55889, Test acc: 72.63%\n",
      "Epoch: 460 | Loss: 0.58488, Accuracy: 66.99% | Test loss: 0.55829, Test acc: 72.63%\n",
      "Epoch: 470 | Loss: 0.58422, Accuracy: 66.99% | Test loss: 0.55768, Test acc: 72.63%\n",
      "Epoch: 480 | Loss: 0.58357, Accuracy: 66.85% | Test loss: 0.55708, Test acc: 72.07%\n",
      "Epoch: 490 | Loss: 0.58292, Accuracy: 66.85% | Test loss: 0.55648, Test acc: 72.07%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(88) # semilla aleatoria\n",
    "\n",
    "# N√∫mero de epocas\n",
    "epochs = 500\n",
    "\n",
    "# Poner los datos en el hardware target\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Loop de training y eval\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_1.train()\n",
    "\n",
    "    # 1. Forward propagation (el modelo regresa logits)\n",
    "    y_logits = model_1(X_train).squeeze() # squeeze para remover `1` dimension extra\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Se calcula loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss acepta los logits de salida\n",
    "                   y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Zero grad para el optimizador\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizador\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Evaluacion\n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward \n",
    "        test_logits = model_1(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print cada 10 epocas\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelV2(\n",
      "  (layer_1): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (layer_2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (layer_3): Linear(in_features=20, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=10, out_features=20)\n",
    "        self.layer_2 = nn.Linear(in_features=20, out_features=20)\n",
    "        self.layer_3 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.relu = nn.ReLU() # <- Se a√±ade funci√≥n de activaci√≥n ReLU\n",
    "        # Tambi√©n se puede usar sigmoid, pero se tendr√≠a que quitar en la parte de transformaci√≥n del output \n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "      # ReLU se aplica entre capas\n",
    "       return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
    "\n",
    "model_3 = ModelV2().to(device)\n",
    "print(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.58228, Accuracy: 66.85% | Test loss: 0.55589, Test acc: 72.63%\n",
      "Epoch: 10 | Loss: 0.58163, Accuracy: 66.85% | Test loss: 0.55529, Test acc: 72.63%\n",
      "Epoch: 20 | Loss: 0.58099, Accuracy: 66.85% | Test loss: 0.55470, Test acc: 73.18%\n",
      "Epoch: 30 | Loss: 0.58035, Accuracy: 66.85% | Test loss: 0.55411, Test acc: 73.18%\n",
      "Epoch: 40 | Loss: 0.57971, Accuracy: 66.99% | Test loss: 0.55352, Test acc: 73.18%\n",
      "Epoch: 50 | Loss: 0.57908, Accuracy: 66.99% | Test loss: 0.55293, Test acc: 73.18%\n",
      "Epoch: 60 | Loss: 0.57845, Accuracy: 66.99% | Test loss: 0.55235, Test acc: 73.74%\n",
      "Epoch: 70 | Loss: 0.57782, Accuracy: 67.13% | Test loss: 0.55177, Test acc: 73.74%\n",
      "Epoch: 80 | Loss: 0.57719, Accuracy: 67.28% | Test loss: 0.55119, Test acc: 73.74%\n",
      "Epoch: 90 | Loss: 0.57656, Accuracy: 67.42% | Test loss: 0.55061, Test acc: 73.74%\n",
      "Epoch: 100 | Loss: 0.57594, Accuracy: 67.56% | Test loss: 0.55004, Test acc: 73.74%\n",
      "Epoch: 110 | Loss: 0.57532, Accuracy: 67.56% | Test loss: 0.54947, Test acc: 73.74%\n",
      "Epoch: 120 | Loss: 0.57470, Accuracy: 67.56% | Test loss: 0.54890, Test acc: 73.74%\n",
      "Epoch: 130 | Loss: 0.57408, Accuracy: 67.56% | Test loss: 0.54834, Test acc: 73.74%\n",
      "Epoch: 140 | Loss: 0.57347, Accuracy: 67.70% | Test loss: 0.54778, Test acc: 72.63%\n",
      "Epoch: 150 | Loss: 0.57286, Accuracy: 67.70% | Test loss: 0.54722, Test acc: 72.63%\n",
      "Epoch: 160 | Loss: 0.57225, Accuracy: 67.70% | Test loss: 0.54666, Test acc: 72.63%\n",
      "Epoch: 170 | Loss: 0.57164, Accuracy: 67.70% | Test loss: 0.54611, Test acc: 73.18%\n",
      "Epoch: 180 | Loss: 0.57104, Accuracy: 67.70% | Test loss: 0.54556, Test acc: 73.74%\n",
      "Epoch: 190 | Loss: 0.57043, Accuracy: 67.84% | Test loss: 0.54502, Test acc: 73.74%\n",
      "Epoch: 200 | Loss: 0.56983, Accuracy: 67.84% | Test loss: 0.54448, Test acc: 73.74%\n",
      "Epoch: 210 | Loss: 0.56924, Accuracy: 67.84% | Test loss: 0.54394, Test acc: 73.74%\n",
      "Epoch: 220 | Loss: 0.56864, Accuracy: 67.84% | Test loss: 0.54341, Test acc: 73.74%\n",
      "Epoch: 230 | Loss: 0.56805, Accuracy: 67.84% | Test loss: 0.54287, Test acc: 73.74%\n",
      "Epoch: 240 | Loss: 0.56746, Accuracy: 67.84% | Test loss: 0.54235, Test acc: 73.74%\n",
      "Epoch: 250 | Loss: 0.56687, Accuracy: 67.98% | Test loss: 0.54182, Test acc: 73.18%\n",
      "Epoch: 260 | Loss: 0.56629, Accuracy: 67.98% | Test loss: 0.54130, Test acc: 73.18%\n",
      "Epoch: 270 | Loss: 0.56570, Accuracy: 67.98% | Test loss: 0.54079, Test acc: 73.18%\n",
      "Epoch: 280 | Loss: 0.56512, Accuracy: 67.98% | Test loss: 0.54028, Test acc: 73.18%\n",
      "Epoch: 290 | Loss: 0.56455, Accuracy: 67.98% | Test loss: 0.53977, Test acc: 73.18%\n",
      "Epoch: 300 | Loss: 0.56397, Accuracy: 67.98% | Test loss: 0.53926, Test acc: 73.18%\n",
      "Epoch: 310 | Loss: 0.56340, Accuracy: 67.98% | Test loss: 0.53876, Test acc: 73.18%\n",
      "Epoch: 320 | Loss: 0.56283, Accuracy: 67.98% | Test loss: 0.53826, Test acc: 73.18%\n",
      "Epoch: 330 | Loss: 0.56227, Accuracy: 68.12% | Test loss: 0.53777, Test acc: 73.18%\n",
      "Epoch: 340 | Loss: 0.56171, Accuracy: 68.12% | Test loss: 0.53728, Test acc: 74.86%\n",
      "Epoch: 350 | Loss: 0.56115, Accuracy: 68.12% | Test loss: 0.53679, Test acc: 74.86%\n",
      "Epoch: 360 | Loss: 0.56059, Accuracy: 68.12% | Test loss: 0.53631, Test acc: 74.30%\n",
      "Epoch: 370 | Loss: 0.56004, Accuracy: 68.12% | Test loss: 0.53583, Test acc: 74.30%\n",
      "Epoch: 380 | Loss: 0.55949, Accuracy: 68.26% | Test loss: 0.53536, Test acc: 74.30%\n",
      "Epoch: 390 | Loss: 0.55894, Accuracy: 68.26% | Test loss: 0.53489, Test acc: 74.30%\n",
      "Epoch: 400 | Loss: 0.55839, Accuracy: 68.54% | Test loss: 0.53442, Test acc: 74.30%\n",
      "Epoch: 410 | Loss: 0.55785, Accuracy: 68.54% | Test loss: 0.53395, Test acc: 74.30%\n",
      "Epoch: 420 | Loss: 0.55731, Accuracy: 68.54% | Test loss: 0.53349, Test acc: 74.30%\n",
      "Epoch: 430 | Loss: 0.55678, Accuracy: 68.54% | Test loss: 0.53304, Test acc: 74.86%\n",
      "Epoch: 440 | Loss: 0.55625, Accuracy: 68.54% | Test loss: 0.53258, Test acc: 74.30%\n",
      "Epoch: 450 | Loss: 0.55572, Accuracy: 68.54% | Test loss: 0.53213, Test acc: 74.86%\n",
      "Epoch: 460 | Loss: 0.55519, Accuracy: 68.54% | Test loss: 0.53169, Test acc: 74.86%\n",
      "Epoch: 470 | Loss: 0.55467, Accuracy: 68.54% | Test loss: 0.53124, Test acc: 74.86%\n",
      "Epoch: 480 | Loss: 0.55415, Accuracy: 68.54% | Test loss: 0.53080, Test acc: 75.42%\n",
      "Epoch: 490 | Loss: 0.55363, Accuracy: 68.54% | Test loss: 0.53037, Test acc: 75.42%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(88) # semilla aleatoria\n",
    "\n",
    "# N√∫mero de epocas\n",
    "epochs = 500\n",
    "\n",
    "# Poner los datos en el hardware target\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Loop de training y eval\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_1.train()\n",
    "\n",
    "    # 1. Forward propagation (el modelo regresa logits)\n",
    "    y_logits = model_1(X_train).squeeze() # squeeze para remover `1` dimension extra\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Se calcula loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss acepta los logits de salida\n",
    "                   y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Zero grad para el optimizador\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizador\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Evaluacion\n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward \n",
    "        test_logits = model_1(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print cada 10 epocas\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
